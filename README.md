<img width="1011" height="509" alt="图片" src="https://github.com/user-attachments/assets/5785f8fa-66fc-423b-89a6-104e7a9f1228" />
180词完整摘要：Multimodal sentiment analysis (MSA) combines text, audio, and visual cues to better understand human emotions. However, textual inputs are prone to various types of noise due to colloquial speech and automatic speech recognition (ASR) transcriptions—yet remains dominant in most models. Human typically resolve speech ambiguity through a learning-before-correction strategy. They first learn to align facial expressions, vocal tone, and speech content. With enough experience,they use this knowledge to correct or complement what they heard with multimodal-guided inference. To mimic this process, we propose SDHM (Staged Diffusion with Hybrid Mixture-of-Experts), a two-stage framework. On the first stage, SDHM leverages Hybrid MoE (alternating linear-MoE and transformer-MoE layers) to progressively enhance feature representations. These enriched semantic features provide a strong foundation for a diffusion-based reconstruction loss, which guides the model in aligning multimodal cues and inferring context-aware meanings. On the second stage, after the model has acquired sufficient "emotional intelligence", the reconstructed features are fused with the original textual inputs to form a more robust primary modality. Random modality missing experiments on MOSI and SIMS achieve SOTA results, confirming SDHM's robustness to noise.
